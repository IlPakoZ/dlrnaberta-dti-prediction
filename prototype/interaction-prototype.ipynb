{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-28T14:14:57.348123Z",
     "iopub.status.busy": "2024-10-28T14:14:57.347821Z",
     "iopub.status.idle": "2024-10-28T14:14:58.332876Z",
     "shell.execute_reply": "2024-10-28T14:14:58.331785Z",
     "shell.execute_reply.started": "2024-10-28T14:14:57.348090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizerFast, RobertaModel, DefaultDataCollator, DataCollatorWithPadding, DataCollatorForLanguageModeling, Trainer, LongformerForMaskedLM,  LongformerTokenizerFast\n",
    "from transformers import TrainingArguments, HfArgumentParser, AutoModelForMaskedLM, AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from torchmetrics import AUROC, AveragePrecision\n",
    "from transformers import RobertaConfig\n",
    "from tokenizers.implementations import CharBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from safetensors.torch import load_file\n",
    "import threading\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract FASTA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -O interaction.protein.gz  \"https://rest.uniprot.org/idmapping/uniprotkb/results/stream/b36da80dd9f77ce4d631842f8d85fe80785b0702?compressed=true&format=fasta\"\n",
    "#!gzip -df \"result.fasta.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset\n",
    "!mkdir dataset/train\n",
    "!mkdir dataset/val\n",
    "!mkdir dataset/test\n",
    "!mkdir aminobert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining with MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T14:16:06.488824Z",
     "iopub.status.busy": "2024-10-28T14:16:06.488455Z",
     "iopub.status.idle": "2024-10-28T14:16:21.325314Z",
     "shell.execute_reply": "2024-10-28T14:16:21.324544Z",
     "shell.execute_reply.started": "2024-10-28T14:16:06.488781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable XLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T14:16:21.326998Z",
     "iopub.status.busy": "2024-10-28T14:16:21.326402Z",
     "iopub.status.idle": "2024-10-28T14:16:21.330827Z",
     "shell.execute_reply": "2024-10-28T14:16:21.329817Z",
     "shell.execute_reply.started": "2024-10-28T14:16:21.326963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "#device = xm.xla_device()\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize an Argument Parser for transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "parser = HfArgumentParser((TrainingArguments, ModelArgs,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tokenize function and pretraining and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T15:11:37.881111Z",
     "iopub.status.busy": "2024-10-28T15:11:37.880346Z",
     "iopub.status.idle": "2024-10-28T15:11:37.930172Z",
     "shell.execute_reply": "2024-10-28T15:11:37.929210Z",
     "shell.execute_reply.started": "2024-10-28T15:11:37.881069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(tokenizer, examples):\n",
    "    return tokenizer(examples[\"fasta\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n",
    "    data_files = {\"val\": args.val_datapath}\n",
    "    \n",
    "    if eval_only:\n",
    "        data_files[\"train\"] = data_files[\"val\"]\n",
    "    else:\n",
    "        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n",
    "        data_files[\"train\"] = args.train_datapath\n",
    "\n",
    "    datasets = load_dataset(\"csv\", data_files=data_files)\n",
    "    datasets = datasets.map(lambda x: tokenize_function(tokenizer, x), batched=True)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n",
    "                      train_dataset=datasets[\"train\"], eval_dataset=datasets[\"val\"],)\n",
    "        \n",
    "    print(\"I'm evaluating...\")\n",
    "    eval_loss = trainer.evaluate()\n",
    "    eval_loss = eval_loss['eval_loss']\n",
    "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
    "\n",
    "    print(f\"Initial eval bpc: {eval_loss/math.log(2)}\")\n",
    "\n",
    "    if not eval_only:\n",
    "        print(\"I'm training...\")\n",
    "        trainer.train(model_path=model_path)\n",
    "        trainer.save_model()\n",
    "\n",
    "        eval_loss = trainer.evaluate()\n",
    "        eval_loss = eval_loss['eval_loss']\n",
    "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')\n",
    "        print(f\"Eval bpc after pretraining: {eval_loss/math.log(2)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define arguments for pretraining (TrainingArguments)\n",
    "Temporary parameters. In the real model, parameters need to be \"optimized\", or in any case chosen using appropriate heuristics.<br>\n",
    "Scaling laws will be used to calculate the optimal amount of training steps/parameters. Learning rate, cosine annealing cycle length and decay and other information on choosing hyperparameters can be found in this paper https://arxiv.org/pdf/2203.15556 and this paper https://openreview.net/pdf?id=Bx6qKuBM2AD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args, model_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
    "    '--output_dir', 'tmp',\n",
    "    '--learning_rate', '2e-4',\n",
    "    '--weight_decay', '0.01',\n",
    "    '--adam_epsilon', '1e-6',\n",
    "    '--max_steps', '5000',\n",
    "    '--logging_steps', '100',\n",
    "    '--save_steps', '500',\n",
    "    '--max_grad_norm', '5.0',\n",
    "    '--per_device_eval_batch_size', '8',\n",
    "    '--per_device_train_batch_size', '2',  # 32GB gpu with fp32\n",
    "    '--gradient_accumulation_steps', '32',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "    #'--num_train_epochs', '1',\n",
    "    '--tpu_num_cores', '8',                      # Number of TPU cores (typically 8)\n",
    "    '--lr_scheduler_type', 'cosine',\n",
    "    '--warmup_ratio', '0.1',\n",
    "    # This cosine scheduler drops to min lr rate of zero, not of 10x less the initial lr like in the paper\n",
    "\n",
    "    #'--lr_scheduler_kwargs', '{\"num_cycles\": 0.5}',           \n",
    "    # This drops approximately 10x  \n",
    "    '--lr_scheduler_kwargs', '{\"num_cycles\": 0.41}',            \n",
    "\n",
    "    #'--warmup_steps', '500',\n",
    "\n",
    "])\n",
    "\n",
    "training_args.train_datapath = './dataset/train/train_proteins.csv'\n",
    "training_args.val_datapath = './dataset/val/val_proteins.csv'\n",
    "\n",
    "training_args.prediction_loss_only = True\n",
    "print(\"Device:\", training_args.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T17:34:35.411436Z",
     "iopub.status.busy": "2024-10-29T17:34:35.410542Z",
     "iopub.status.idle": "2024-10-29T17:34:40.903596Z",
     "shell.execute_reply": "2024-10-29T17:34:40.902229Z",
     "shell.execute_reply.started": "2024-10-29T17:34:35.411379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------  Train tokenizer on DNA sequences --------------------------\n",
    "#files = ['./dataset/train/tokenizer_train.txt'] \n",
    "#roberta_base_tokenizer = CharBPETokenizer()\n",
    "\n",
    "# Customize training (change vocab size)\n",
    "#roberta_base_tokenizer.train(files=files, vocab_size=500, min_frequency=2, special_tokens=[\n",
    "#    \"<s>\",\n",
    "#    \"<pad>\",\n",
    "#    \"</s>\",\n",
    "#    \"<unk>\",\n",
    "#    \"<mask>\",\n",
    "#])\n",
    "\n",
    "#roberta_base_tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#    (\"</s>\", roberta_base_tokenizer.token_to_id(\"</s>\")),\n",
    "#    (\"<s>\", roberta_base_tokenizer.token_to_id(\"<s>\")),\n",
    "#)\n",
    "\n",
    "#roberta_base_tokenizer.enable_truncation(max_length=512)\n",
    "#roberta_base_tokenizer.save_model(\"./aminobert\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------  Define model and resize token embeddings ---------------------\n",
    "\n",
    "roberta_base_tokenizer = RobertaTokenizerFast.from_pretrained(\"./aminobert\")\n",
    "\n",
    "roberta_base = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "roberta_base.resize_token_embeddings(len(roberta_base_tokenizer))\n",
    "print(f\"Roberta parameters: {int(roberta_base.num_parameters()/1000000)}M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a smaller version of RoBERTa (for testing) and pretrain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T15:11:47.748567Z",
     "iopub.status.busy": "2024-10-28T15:11:47.747683Z",
     "iopub.status.idle": "2024-10-28T15:11:47.829722Z",
     "shell.execute_reply": "2024-10-28T15:11:47.828779Z",
     "shell.execute_reply.started": "2024-10-28T15:11:47.748527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.roberta.encoder.layer\n",
    "    newModuleList = nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(0, num_layers_to_keep):\n",
    "        newModuleList.append(oldModuleList[i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.roberta.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_roberta = deleteEncodingLayers(roberta_base,4)\n",
    "print(f\"Small roberta parameters: {int(small_roberta.num_parameters()/1000000)}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T15:11:50.105149Z",
     "iopub.status.busy": "2024-10-28T15:11:50.104763Z",
     "iopub.status.idle": "2024-10-28T16:44:03.217092Z",
     "shell.execute_reply": "2024-10-28T16:44:03.216169Z",
     "shell.execute_reply.started": "2024-10-28T15:11:50.105112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pretrain_and_evaluate(training_args, small_roberta, roberta_base_tokenizer, eval_only=False, model_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T16:49:29.579774Z",
     "iopub.status.busy": "2024-10-28T16:49:29.579037Z",
     "iopub.status.idle": "2024-10-28T16:49:30.440253Z",
     "shell.execute_reply": "2024-10-28T16:49:30.439269Z",
     "shell.execute_reply.started": "2024-10-28T16:49:29.579714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assuming `model` is your pretrained model and `tokenizer` is your tokenizer\n",
    "#output_dir = \"./pretrained\"\n",
    "\n",
    "# Save the model\n",
    "#small_roberta.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "#roberta_base_tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining with Functional Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for sequence classification\n",
    "model_encoder = AutoModelForMaskedLM.from_pretrained(\"./pretrained\")\n",
    "model_encoder = deleteEncodingLayers(model_encoder,4).roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InputDataset(Dataset):\n",
    "    def __init__(self, interactions_file, tokenizer1, tokenizer2):\n",
    "        interactions = pd.read_csv(interactions_file)\n",
    "        \n",
    "        if interactions.shape[1] == 3:\n",
    "            self.targets = torch.tensor(interactions.iloc[:,2].values, dtype=torch.long)\n",
    "        elif interactions.shape[1] != 2:\n",
    "            raise Exception(f\"Invalid input file: input should have shape (N, 3) or (N, 2), but has shape {self.interactions.shape}.\")\n",
    "        \n",
    "        self.inputs = interactions.iloc[:,:2].values\n",
    "        self.tokenizer1 = tokenizer1\n",
    "        self.tokenizer2 = tokenizer2\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        input1 = self.tokenizer1(list(np.atleast_1d(self.inputs[idx, 0])),\n",
    "                            padding=\"max_length\", \n",
    "                            truncation=True, \n",
    "                            max_length=512,\n",
    "                            return_tensors=\"pt\")\n",
    "        input2 = self.tokenizer2(list(np.atleast_1d(self.inputs[idx, 1])),\n",
    "                            padding=\"max_length\", \n",
    "                            truncation=True, \n",
    "                            max_length=512,\n",
    "                            return_tensors=\"pt\")\n",
    "        \n",
    "        input1[\"input_ids\"] = input1[\"input_ids\"].view(-1)\n",
    "        input2[\"input_ids\"] = input2[\"input_ids\"].view(-1)\n",
    "\n",
    "\n",
    "        return ((input1, input2), self.targets[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN Interaction Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionModelFFN(nn.Module):\n",
    "    def __init__(self, encoder, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linear_gelu_stack = nn.Sequential(\n",
    "            nn.Linear(1536, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, num_labels),\n",
    "        ) \n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        y1 = self.encoder(**x1).last_hidden_state\n",
    "        y2 = self.encoder(**x2).last_hidden_state\n",
    "        z = torch.concatenate([y1.mean(dim=1), y2.mean(dim=1)], dim=1)\n",
    "        y = self.linear_gelu_stack(z)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention Interaction Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key is the drug because it binds to the target (like the key of a lock)\n",
    "\n",
    "class InteractionModelATTN(nn.Module):\n",
    "    def __init__(self, encoder, num_labels, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.multihead_attention = nn.MultiheadAttention(768, num_heads, dropout = 0.2, batch_first=True)\n",
    "\n",
    "        self.output = nn.Linear(768, num_labels)\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=512, out_channels=1, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        y1 = self.encoder(**x1).last_hidden_state       # The target in the real case\n",
    "        y2 = self.encoder(**x2).last_hidden_state       # The drug in the real case\n",
    "        out, _ = self.multihead_attention(y1, y2, y1)\n",
    "\n",
    "        out = self.norm(out)\n",
    "                \n",
    "        out = torch.squeeze(self.conv(out), axis=1)\n",
    "        out = self.output(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer1 = RobertaTokenizerFast.from_pretrained(\"./aminobert\")\n",
    "tokenizer2 = RobertaTokenizerFast.from_pretrained(\"./aminobert\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tokenizer1(\"ACGCGCAG\", max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
    "res1 = tokenizer1([\"ACGCGCAG\", \"DACDAFV\"] , max_length=512,  padding=\"max_length\", return_tensors=\"pt\")\n",
    "res2 = tokenizer2([\"ACGCGCCDFAG\", \"CADCAEDF\"] , max_length=512,  padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "multihead_attention = nn.MultiheadAttention(768, 1, dropout = 0.2, batch_first=True)\n",
    "model_encoder.eval()\n",
    "p1 = model_encoder(**res1).last_hidden_state\n",
    "p2 = model_encoder(**res2).last_hidden_state\n",
    "\n",
    "att = multihead_attention(p1, p2, p1)[0]\n",
    "\n",
    "norm = nn.LayerNorm(768)\n",
    "out = norm(att)\n",
    "conv = nn.Conv1d(in_channels=512, out_channels=1, kernel_size=1)\n",
    "print(list(conv.parameters())[0].shape)\n",
    "out = torch.squeeze(conv(out), axis=1)\n",
    "print(out.shape)\n",
    "\n",
    "#z = torch.concatenate([p1.mean(dim=1), p2.mean(dim=1)], dim=1)\n",
    "\n",
    "#print(z.shape)\n",
    "#model_encoder.config\n",
    "\n",
    "\n",
    "#datasets = datasets.map(lambda x: finetune_tokenize(tokenizer, x), batched=True)\n",
    "#tokenizer(datasets[\"train\"][\"text\"][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_interactions_file = \"dataset/train/train_inters.csv\"\n",
    "val_interactions_file = \"dataset/val/val_inters.csv\"\n",
    "\n",
    "train_parameters = {\"train_batch_size\": 4,\n",
    "                    \"device\": \"cuda\",\n",
    "                    \"learning_rate\": 2e-5,\n",
    "                    \"adam_epsilon\": 1e-6,\n",
    "                    \"gradient_accumulation_steps\": 32,\n",
    "                    \"num_training_steps\":3000,\n",
    "                    \"log_performance_every\":20,\n",
    "                    }\n",
    "\n",
    "train_dataset = InputDataset(train_interactions_file, tokenizer1, tokenizer2)\n",
    "val_dataset = InputDataset(val_interactions_file, tokenizer1, tokenizer2)\n",
    "\n",
    "def evaluate_model(cpu_model, data_loader, thread_id):\n",
    "    cpu_model.eval() \n",
    "\n",
    "    auroc_metric = AUROC(task=\"multiclass\", num_classes=2)\n",
    "    auprc_metric = AveragePrecision(task=\"multiclass\", num_classes=2)\n",
    "    \n",
    "    eval_n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for source, targets in data_loader:\n",
    "      \n",
    "            if eval_n > 150:\n",
    "                break\n",
    "                        \n",
    "            output = cpu_model(source[0], source[1])\n",
    "            auroc_metric.update(output, targets)\n",
    "            auprc_metric.update(output, targets)\n",
    "\n",
    "            eval_n += 1\n",
    "\n",
    "    auroc_score = auroc_metric.compute()\n",
    "    auprc_score = auprc_metric.compute()\n",
    "    \n",
    "    print(f\"Step {thread_id} - AUROC val: {auroc_score:.4f}, AUPRC val: {auprc_score:.4f}\")\n",
    "      \n",
    "#add weight decay\n",
    "def finetune_and_evaluate(model):\n",
    "    deepspeed_plugin = DeepSpeedPlugin(zero_stage=3, \n",
    "                                       offload_param_device = \"cpu\",\n",
    "                                        offload_optimizer_device = \"cpu\"\n",
    "                                       )\n",
    "    accelerator = Accelerator(deepspeed_plugin=deepspeed_plugin, gradient_accumulation_steps=train_parameters[\"gradient_accumulation_steps\"])\n",
    "    device = accelerator.device\n",
    "\n",
    "    auroc_metric_train = AUROC(task=\"multiclass\", num_classes=2).to(device)\n",
    "    auprc_metric_train = AveragePrecision(task=\"multiclass\", num_classes=2).to(device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=train_parameters[\"learning_rate\"], weight_decay=0.01)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_parameters[\"train_batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=True)\n",
    "    with torch.no_grad():\n",
    "        for source, targets in val_loader:\n",
    "            source1, targets1 = source, targets\n",
    "            break\n",
    "\n",
    "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n",
    "    print(\"DEVICE USED:\", accelerator.device)\n",
    "    print(\"NUM PROCESSES:\", accelerator.num_processes)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    step = 0  # Initialize the step variable\n",
    "    epoch = 1\n",
    "    max_steps = train_parameters[\"num_training_steps\"]\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    mean_loss = 0\n",
    "\n",
    "    while step < max_steps:\n",
    "        print(f\"EPOCH {epoch}:\")\n",
    "        for train_source, train_targets in train_loader:\n",
    "            with accelerator.accumulate(model):\n",
    "                output = model(train_source[0], train_source[1])\n",
    "                loss = criterion(output, train_targets)\n",
    "                #print(loss)\n",
    "                grad_mean = []\n",
    "                mean_loss+=loss.item()\n",
    "                accelerator.backward(loss)\n",
    "                auroc_metric_train.update(output, train_targets)\n",
    "                auprc_metric_train.update(output, train_targets)\n",
    "\n",
    "                # Compute metrics at a frequency of your choice (e.g., every 5 steps)\n",
    "                if (step + 1) % (train_parameters[\"log_performance_every\"]*train_parameters[\"gradient_accumulation_steps\"]) == 0:\n",
    "                    # compute metrics for validation set    \n",
    "\n",
    "                    cpu_model = copy.deepcopy(model).cpu()\n",
    "                    output = cpu_model(source1[0], source1[1])\n",
    "                    print(output.detach(), targets1.detach())\n",
    "                    eval_thread = threading.Thread(\n",
    "                        target=evaluate_model, \n",
    "                        args=(cpu_model, val_loader, step+1)\n",
    "                    )\n",
    "                    eval_thread.start()\n",
    "                    \n",
    "                    auroc_score_train = auroc_metric_train.compute()\n",
    "                    auprc_score_train = auprc_metric_train.compute()\n",
    "                    auroc_metric_train.reset()\n",
    "                    auroc_metric_train.reset()\n",
    "\n",
    "\n",
    "            \n",
    "                optimizer.step()\n",
    "                \n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        grad_mean.append(param.grad.mean().cpu().detach())\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "                   \n",
    "\n",
    "                if (step + 1) % (train_parameters[\"log_performance_every\"]*train_parameters[\"gradient_accumulation_steps\"]) == 0:\n",
    "                    mean_loss/= (train_parameters[\"log_performance_every\"]*train_parameters[\"gradient_accumulation_steps\"])\n",
    "                    print(f\"Step {step + 1} - Loss: {mean_loss:.4f}, AUROC: {auroc_score_train:.4f}, AUPRC: {auprc_score_train:.4f}\")\n",
    "                    mean_loss = 0\n",
    "                    plt.yscale(\"log\")\n",
    "\n",
    "                    plt.plot(grad_mean)\n",
    "                    plt.show()\n",
    "\n",
    "                step += 1\n",
    "\n",
    "                if step >= max_steps:\n",
    "                    break\n",
    "        epoch+=1\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_encoder1 = AutoModelForMaskedLM.from_pretrained(\"./pretrained\")\n",
    "#model_encoder1 = deleteEncodingLayers(model_encoder1,4).roberta\n",
    "#model_encoder2 = AutoModelForMaskedLM.from_pretrained(\"./pretrained\")\n",
    "#model_encoder2 = deleteEncodingLayers(model_encoder2,4).roberta\n",
    "\n",
    "model_encoder1 = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "model_encoder1.resize_token_embeddings(len(tokenizer1))\n",
    "model_encoder1 = deleteEncodingLayers(model_encoder1,2).roberta\n",
    "model_encoder2 = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "model_encoder2.resize_token_embeddings(len(tokenizer1))\n",
    "model_encoder2 = deleteEncodingLayers(model_encoder2,2).roberta\n",
    "\n",
    "\n",
    "\n",
    "models = [InteractionModelFFN(model_encoder1, 2), InteractionModelATTN(model_encoder2,2)][1:]\n",
    "\n",
    "for model in models:\n",
    "   finetune_and_evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-28T14:16:26.269012Z",
     "iopub.status.idle": "2024-10-28T14:16:26.269351Z",
     "shell.execute_reply": "2024-10-28T14:16:26.269194Z",
     "shell.execute_reply.started": "2024-10-28T14:16:26.269176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "longformer = LongformerForMaskedLM.from_pretrained('allenai/longformer-base-4096')\n",
    "longformer_base_tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-28T14:16:26.270269Z",
     "iopub.status.idle": "2024-10-28T14:16:26.270602Z",
     "shell.execute_reply": "2024-10-28T14:16:26.270450Z",
     "shell.execute_reply.started": "2024-10-28T14:16:26.270432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pretrain_and_evaluate(training_args, longformer, longformer_base_tokenizer, eval_only=False, model_path=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(train_interactions_file)\n",
    "val_dataset = pd.read_csv(val_interactions_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"min_samples_split\": trial.suggest_float(\"min_samples_split\", 0.1, 1),\n",
    "        \"random_state\": 14\n",
    "    }\n",
    "    train_dataset = pd.read_csv(train_interactions_file)\n",
    "    val_dataset = pd.read_csv(val_interactions_file)\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(train_x, train_targets)\n",
    "\n",
    "    score_metric = AveragePrecision(task=\"multiclass\", num_classes=2)\n",
    "    \n",
    "    # In binary classification, the probability of the class with the “greater label” should be provided. \n",
    "    # The “greater label” corresponds to classifier.classes_[1] and thus classifier.predict_proba(X)[:, 1].\n",
    "    # See https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case\n",
    "    score = score_metric.update(val_inputs, val_outputs)\n",
    "    return score\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=14),\n",
    "    study_name=\"random_forest_feature_select\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=10)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5959684,
     "sourceId": 9737243,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-28T14:14:57.348123Z",
     "iopub.status.busy": "2024-10-28T14:14:57.347821Z",
     "iopub.status.idle": "2024-10-28T14:14:58.332876Z",
     "shell.execute_reply": "2024-10-28T14:14:58.331785Z",
     "shell.execute_reply.started": "2024-10-28T14:14:57.348090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizerFast, RobertaModel, DefaultDataCollator, DataCollatorWithPadding, DataCollatorForLanguageModeling, Trainer, LongformerForMaskedLM,  LongformerTokenizerFast\n",
    "from transformers import TrainingArguments, HfArgumentParser, AutoModelForMaskedLM, AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from torchmetrics import AUROC, AveragePrecision\n",
    "from transformers import RobertaConfig\n",
    "from tokenizers.implementations import CharBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from safetensors.torch import load_file\n",
    "import threading\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract FASTA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -O interaction.protein.gz  \"https://rest.uniprot.org/idmapping/uniprotkb/results/stream/b36da80dd9f77ce4d631842f8d85fe80785b0702?compressed=true&format=fasta\"\n",
    "#!gzip -df \"result.fasta.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset\n",
    "!mkdir dataset/train\n",
    "!mkdir dataset/val\n",
    "!mkdir dataset/test\n",
    "!mkdir aminobert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and extraction of DNA sequences from FASTA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sequences from fasta files\n",
    "def preprocess_fasta(file_name):\n",
    "    fastas = []\n",
    "    with open(file_name, \"r\") as f:\n",
    "        fasta_sequence = \"\"\n",
    "        while(True):\n",
    "            line1 = f.readline()\n",
    "            if not line1:\n",
    "                break\n",
    "            if line1[0] == \">\":\n",
    "                fastas.append(fasta_sequence)\n",
    "                fasta_sequence = \"\"\n",
    "                continue\n",
    "            fasta_sequence += line1.strip()\n",
    "            \n",
    "    fastas.append(fasta_sequence)\n",
    "    return fastas[1:]\n",
    "\n",
    "fastas = preprocess_fasta(\"result.fasta\")\n",
    "#fastas = fastas[:len(fastas)\\\\10]\n",
    "train, val = train_test_split(fastas, train_size = 0.8)\n",
    "train, test = train_test_split(train, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write DNA sequences to files as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sequence data to file (dataset)\n",
    "def write_to_file(file, sequences):\n",
    "    with open(file, \"w\", encoding='utf-8') as f:\n",
    "        for seq in sequences:\n",
    "            f.write(seq)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "write_to_file(\"dataset/train/data.txt\", train)\n",
    "write_to_file(\"dataset/test/data.txt\", test)\n",
    "write_to_file(\"dataset/val/data.txt\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create interaction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notfound = \"data/9606.notfound.txt\"\n",
    "idfile = \"data/9606.proteins.0.9.txt\"\n",
    "newids = \"proteinids.txt\"\n",
    "positive_interact = \"data/9606.links.0.9.txt\"\n",
    "negative_interact = \"data/9606.negatives.0.9.txt\"\n",
    "\n",
    "with open(notfound) as f:\n",
    "    notprotids = f.readlines()\n",
    "\n",
    "with open(idfile) as f:\n",
    "    protids = f.readlines()\n",
    "\n",
    "for notid in notprotids:\n",
    "    protids.remove(notid)\n",
    "\n",
    "with open(newids, \"w\") as f:\n",
    "    f.writelines(protids)\n",
    "\n",
    "notprotids = list(map(lambda x:x.strip(), notprotids))\n",
    "pos_inter = []\n",
    "with open(\"positive_inter.txt\", \"w\") as w:\n",
    "    with open(positive_interact, \"r\") as f:\n",
    "        for row in f.readlines():\n",
    "            els = row.strip().split()[:2]\n",
    "            if els[0] in notprotids:\n",
    "                continue\n",
    "            if els[1] in notprotids:\n",
    "                continue\n",
    "            pos_inter.append(row)\n",
    "    w.writelines(\"\".join(pos_inter))\n",
    "        \n",
    "neg_inter = []\n",
    "with open(\"negative_inter.txt\", \"w\") as w:\n",
    "    with open(negative_interact, \"r\") as f:\n",
    "        for row in f.readlines():\n",
    "            els = row.strip().split()[:2]\n",
    "            if els[0] in notprotids:\n",
    "                continue\n",
    "            if els[1] in notprotids:\n",
    "                continue\n",
    "            neg_inter.append(row)\n",
    "        \n",
    "    w.writelines(\"\".join(neg_inter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_interact = \"positive_inter.txt\"\n",
    "neg_interact = \"negative_inter.txt\"\n",
    "\n",
    "with open(newids, \"r\") as f:\n",
    "    protids = list(map(lambda x: x.strip(), f.readlines()))\n",
    "\n",
    "prot1 = preprocess_fasta(\"interaction.protein\")\n",
    "print(len(prot1))\n",
    "\n",
    "data = {\"ID\": protids,\n",
    "        \"FASTA\": prot1}\n",
    "proteins = pd.DataFrame(data)\n",
    "proteins = proteins.set_index(\"ID\")\n",
    "\n",
    "with open(pos_interact, \"r\") as f:\n",
    "        pos_inter = np.array(list(map(lambda x: x.strip().split()[:2], f.readlines())))\n",
    "\n",
    "with open(neg_interact, \"r\") as f:\n",
    "        neg_inter = np.array(list(map(lambda x: x.strip().split()[:2], f.readlines())))\n",
    "\n",
    "#prot2 = preprocess_fasta(\"/kaggle/working/target.fasta\")\n",
    "\n",
    "trainpos, valpos = train_test_split(pos_inter, train_size = 0.8)\n",
    "trainpos, testpos = train_test_split(pos_inter, train_size = 0.8)\n",
    "trainneg, valneg = train_test_split(neg_inter, train_size = 0.8)\n",
    "trainneg, testneg = train_test_split(neg_inter, train_size = 0.8)\n",
    "\n",
    "train = np.concatenate([trainpos, trainneg])\n",
    "val = np.concatenate([valpos, valneg])\n",
    "test = np.concatenate([testpos, testneg])\n",
    "train_prot1 = proteins.loc[train[:,0], \"FASTA\"].values\n",
    "train_prot2 = proteins.loc[train[:,1], \"FASTA\"].values\n",
    "val_prot1 = proteins.loc[val[:,0], \"FASTA\"].values\n",
    "val_prot2 = proteins.loc[val[:,1], \"FASTA\"].values\n",
    "test_prot1 = proteins.loc[test[:,0], \"FASTA\"].values\n",
    "test_prot2 = proteins.loc[test[:,1], \"FASTA\"].values\n",
    "\n",
    "train_labels = [1]*len(trainpos) + [0]*len(trainneg)\n",
    "val_labels = [1]*len(valpos) + [0]*len(valneg)\n",
    "test_labels = [1]*len(testpos) + [0]*len(testneg)\n",
    "\n",
    "with open(\"train_interactions.csv\", \"w\") as f:\n",
    "       f.write(\"fasta1,fasta2,labels\\n\")\n",
    "       for el1, el2, y in zip(train_prot1, train_prot2, train_labels):\n",
    "              f.write(f\"{el1},{el2},{y}\\n\")\n",
    "\n",
    "with open(\"val_interactions.csv\", \"w\") as f:\n",
    "       f.write(\"fasta1,fasta2,labels\\n\")\n",
    "       for el1, el2, y in zip(val_prot1, val_prot2, val_labels):\n",
    "              f.write(f\"{el1},{el2},{y}\\n\")\n",
    "\n",
    "with open(\"test_interactions.csv\", \"w\") as f:\n",
    "       f.write(\"fasta1,fasta2,labels\\n\")\n",
    "       for el1, el2, y in zip(test_prot1, test_prot2, test_labels):\n",
    "              f.write(f\"{el1},{el2},{y}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T14:16:06.488824Z",
     "iopub.status.busy": "2024-10-28T14:16:06.488455Z",
     "iopub.status.idle": "2024-10-28T14:16:21.325314Z",
     "shell.execute_reply": "2024-10-28T14:16:21.324544Z",
     "shell.execute_reply.started": "2024-10-28T14:16:06.488781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable XLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T14:16:21.326998Z",
     "iopub.status.busy": "2024-10-28T14:16:21.326402Z",
     "iopub.status.idle": "2024-10-28T14:16:21.330827Z",
     "shell.execute_reply": "2024-10-28T14:16:21.329817Z",
     "shell.execute_reply.started": "2024-10-28T14:16:21.326963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "#device = xm.xla_device()\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize an Argument Parser for transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "parser = HfArgumentParser((TrainingArguments, ModelArgs,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tokenize function and pretraining and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T15:11:37.881111Z",
     "iopub.status.busy": "2024-10-28T15:11:37.880346Z",
     "iopub.status.idle": "2024-10-28T15:11:37.930172Z",
     "shell.execute_reply": "2024-10-28T15:11:37.929210Z",
     "shell.execute_reply.started": "2024-10-28T15:11:37.881069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(tokenizer, examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n",
    "    data_files = {\"val\": args.val_datapath}\n",
    "    \n",
    "    if eval_only:\n",
    "        data_files[\"train\"] = data_files[\"val\"]\n",
    "    else:\n",
    "        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n",
    "        data_files[\"train\"] = args.train_datapath\n",
    "\n",
    "    datasets = load_dataset(\"text\", data_files=data_files)\n",
    "    datasets = datasets.map(lambda x: tokenize_function(tokenizer, x), batched=True)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n",
    "                      train_dataset=datasets[\"train\"], eval_dataset=datasets[\"val\"],)\n",
    "        \n",
    "    print(\"I'm evaluating...\")\n",
    "    eval_loss = trainer.evaluate()\n",
    "    eval_loss = eval_loss['eval_loss']\n",
    "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
    "\n",
    "    print(f\"Initial eval bpc: {eval_loss/math.log(2)}\")\n",
    "\n",
    "    if not eval_only:\n",
    "        print(\"I'm training...\")\n",
    "        trainer.train(model_path=model_path)\n",
    "        trainer.save_model()\n",
    "\n",
    "        eval_loss = trainer.evaluate()\n",
    "        eval_loss = eval_loss['eval_loss']\n",
    "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')\n",
    "        print(f\"Eval bpc after pretraining: {eval_loss/math.log(2)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define arguments for pretraining (TrainingArguments)\n",
    "Temporary parameters. In the real model, parameters need to be \"optimized\", or in any case chosen using appropriate heuristics.<br>\n",
    "Scaling laws will be used to calculate the optimal amount of training steps/parameters. Learning rate, cosine annealing cycle length and decay and other information on choosing hyperparameters can be found in this paper https://arxiv.org/pdf/2203.15556 and this paper https://openreview.net/pdf?id=Bx6qKuBM2AD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "training_args, model_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
    "    '--output_dir', 'tmp',\n",
    "    '--learning_rate', '2e-4',\n",
    "    '--weight_decay', '0.01',\n",
    "    '--adam_epsilon', '1e-6',\n",
    "    '--max_steps', '2000',\n",
    "    '--logging_steps', '100',\n",
    "    '--save_steps', '500',\n",
    "    '--max_grad_norm', '5.0',\n",
    "    '--per_device_eval_batch_size', '8',\n",
    "    '--per_device_train_batch_size', '2',  # 32GB gpu with fp32\n",
    "    '--gradient_accumulation_steps', '32',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "    #'--num_train_epochs', '1',\n",
    "    '--tpu_num_cores', '8',                      # Number of TPU cores (typically 8)\n",
    "    '--lr_scheduler_type', 'cosine',\n",
    "    '--warmup_ratio', '0.1',\n",
    "    # This cosine scheduler drops to min lr rate of zero, not of 10x less the initial lr like in the paper\n",
    "\n",
    "    #'--lr_scheduler_kwargs', '{\"num_cycles\": 0.5}',           \n",
    "    # This drops approximately 10x  \n",
    "    '--lr_scheduler_kwargs', '{\"num_cycles\": 0.41}',            \n",
    "\n",
    "    #'--warmup_steps', '500',\n",
    "\n",
    "])\n",
    "\n",
    "training_args.val_datapath = './dataset/val/data.txt'\n",
    "training_args.train_datapath = './dataset/train/data.txt'\n",
    "training_args.prediction_loss_only = True\n",
    "print(\"Device:\", training_args.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T17:34:35.411436Z",
     "iopub.status.busy": "2024-10-29T17:34:35.410542Z",
     "iopub.status.idle": "2024-10-29T17:34:40.903596Z",
     "shell.execute_reply": "2024-10-29T17:34:40.902229Z",
     "shell.execute_reply.started": "2024-10-29T17:34:35.411379Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m roberta_base_tokenizer \u001b[38;5;241m=\u001b[39m CharBPETokenizer()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Customize training (change vocab size)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mroberta_base_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<s>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<pad>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m</s>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<unk>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<mask>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#roberta_base_tokenizer._tokenizer.post_processor = BertProcessing(\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#    (\"</s>\", roberta_base_tokenizer.token_to_id(\"</s>\")),\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#    (\"<s>\", roberta_base_tokenizer.token_to_id(\"<s>\")),\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m roberta_base_tokenizer\u001b[38;5;241m.\u001b[39menable_truncation(max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tokenizers/implementations/char_level_bpe.py:121\u001b[0m, in \u001b[0;36mCharBPETokenizer.train\u001b[0;34m(self, files, vocab_size, min_frequency, special_tokens, limit_alphabet, initial_alphabet, suffix, show_progress)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    120\u001b[0m     files \u001b[38;5;241m=\u001b[39m [files]\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------  Train tokenizer on DNA sequences --------------------------\n",
    "files = ['./dataset/train/data.txt','./dataset/val/data.txt'] \n",
    "roberta_base_tokenizer = CharBPETokenizer()\n",
    "\n",
    "# Customize training (change vocab size)\n",
    "roberta_base_tokenizer.train(files=files, vocab_size=1000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "#roberta_base_tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#    (\"</s>\", roberta_base_tokenizer.token_to_id(\"</s>\")),\n",
    "#    (\"<s>\", roberta_base_tokenizer.token_to_id(\"<s>\")),\n",
    "#)\n",
    "\n",
    "roberta_base_tokenizer.enable_truncation(max_length=512)\n",
    "roberta_base_tokenizer.save_model(\"./aminobert\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta parameters: 86M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------  Define model and resize token embeddings ---------------------\n",
    "\n",
    "roberta_base_tokenizer = RobertaTokenizerFast.from_pretrained(\"./aminobert\")\n",
    "\n",
    "roberta_base = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "roberta_base.resize_token_embeddings(len(roberta_base_tokenizer))\n",
    "print(f\"Roberta parameters: {int(roberta_base.num_parameters()/1000000)}M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a smaller version of RoBERTa (for testing) and pretrain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T15:11:47.748567Z",
     "iopub.status.busy": "2024-10-28T15:11:47.747683Z",
     "iopub.status.idle": "2024-10-28T15:11:47.829722Z",
     "shell.execute_reply": "2024-10-28T15:11:47.828779Z",
     "shell.execute_reply.started": "2024-10-28T15:11:47.748527Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small roberta parameters: 43M\n"
     ]
    }
   ],
   "source": [
    "def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model\n",
    "    oldModuleList = model.roberta.encoder.layer\n",
    "    newModuleList = nn.ModuleList()\n",
    "\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(0, num_layers_to_keep):\n",
    "        newModuleList.append(oldModuleList[i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.roberta.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "    \n",
    "small_roberta = deleteEncodingLayers(roberta_base,6)\n",
    "print(f\"Small roberta parameters: {int(small_roberta.num_parameters()/1000000)}M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T15:11:50.105149Z",
     "iopub.status.busy": "2024-10-28T15:11:50.104763Z",
     "iopub.status.idle": "2024-10-28T16:44:03.217092Z",
     "shell.execute_reply": "2024-10-28T16:44:03.216169Z",
     "shell.execute_reply.started": "2024-10-28T15:11:50.105112Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading and tokenizing training data is usually slow: ./dataset/train/data.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc8df71172149cd9d765407f9227305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41010 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6da1f945b9a47789d0176470af40ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm evaluating...\n",
      "Trainer: Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 41010\n",
      "})\n",
      "Eval loop:  False\n",
      "Prediction_loss_only: True\n",
      "True\n",
      "  Num examples = 41010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676eef2ac1c04218aea9186c58c35111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initial eval bpc: 12.693354281302868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output metrics: {'eval_loss': 8.798362731933594, 'eval_runtime': 851.7556, 'eval_samples_per_second': 48.148, 'eval_steps_per_second': 6.019}\n",
      "Initial eval bpc: 12.693354281302868\n",
      "I'm training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pako/.local/lib/python3.10/site-packages/transformers/trainer.py:1828: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b70d55c57274a0a801e3b00c6825a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpretrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmall_roberta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroberta_base_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mpretrain_and_evaluate\u001b[0;34m(args, model, tokenizer, eval_only, model_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m eval_only:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m     32\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrain_and_evaluate(training_args, small_roberta, roberta_base_tokenizer, eval_only=False, model_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T16:49:29.579774Z",
     "iopub.status.busy": "2024-10-28T16:49:29.579037Z",
     "iopub.status.idle": "2024-10-28T16:49:30.440253Z",
     "shell.execute_reply": "2024-10-28T16:49:30.439269Z",
     "shell.execute_reply.started": "2024-10-28T16:49:29.579714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./pretrained/tokenizer_config.json',\n",
       " './pretrained/special_tokens_map.json',\n",
       " './pretrained/vocab.json',\n",
       " './pretrained/merges.txt',\n",
       " './pretrained/added_tokens.json',\n",
       " './pretrained/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `model` is your pretrained model and `tokenizer` is your tokenizer\n",
    "#output_dir = \"./pretrained\"\n",
    "\n",
    "# Save the model\n",
    "#small_roberta.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "#roberta_base_tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at ./pretrained and are newly initialized: ['roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nroberta_base_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\\nroberta_base = RobertaForMaskedLM.from_pretrained('roberta-base')\\nfiles = ['./dataset/train/data.txt','./dataset/val/data.txt'] \\nroberta_base_tokenizer = roberta_base_tokenizer.train_new_from_iterator(files, 1000)\\n\\nroberta_base.resize_token_embeddings(len(roberta_base_tokenizer))\\nsmall_roberta = deleteEncodingLayers(roberta_base,6)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model for sequence classification\n",
    "model_encoder = AutoModelForMaskedLM.from_pretrained(\"./pretrained\")\n",
    "model_encoder = deleteEncodingLayers(model_encoder,6).roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InputDataset(Dataset):\n",
    "    def __init__(self, interactions_file, tokenizer1, tokenizer2):\n",
    "        interactions = pd.read_csv(interactions_file)\n",
    "        \n",
    "        if interactions.shape[1] == 3:\n",
    "            self.targets = torch.tensor(interactions.iloc[:,2].values, dtype=torch.long)\n",
    "        elif interactions.shape[1] != 2:\n",
    "            raise Exception(f\"Invalid input file: input should have shape (N, 3) or (N,2), but has shape {self.interactions.shape}.\")\n",
    "        \n",
    "        self.inputs = interactions.iloc[:,:2].values\n",
    "        self.tokenizer1 = tokenizer1\n",
    "        self.tokenizer2 = tokenizer2\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        input1 = self.tokenizer1(list(np.atleast_1d(self.inputs[idx, 0])),\n",
    "                            padding=\"max_length\", \n",
    "                            truncation=True, \n",
    "                            max_length=512,\n",
    "                            return_tensors=\"pt\")\n",
    "        input2 = self.tokenizer2(list(np.atleast_1d(self.inputs[idx, 1])),\n",
    "                            padding=\"max_length\", \n",
    "                            truncation=True, \n",
    "                            max_length=512,\n",
    "                            return_tensors=\"pt\")\n",
    "        \n",
    "        input1[\"input_ids\"] = input1[\"input_ids\"].view(-1)\n",
    "        input2[\"input_ids\"] = input2[\"input_ids\"].view(-1)\n",
    "\n",
    "\n",
    "        return ((input1, input2), self.targets[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionModel(nn.Module):\n",
    "    def __init__(self, encoder, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linear_gelu_stack = nn.Sequential(\n",
    "            nn.Linear(1536, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, num_labels),\n",
    "        ) \n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        y1 = self.encoder(**x1).last_hidden_state\n",
    "        y2 = self.encoder(**x2).last_hidden_state\n",
    "        z = torch.concatenate([y1.mean(dim=1), y2.mean(dim=1)], dim=1)\n",
    "        y = self.linear_gelu_stack(z)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1536])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"./pretrained\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.41.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 1000\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"./pretrained\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"./pretrained\")\n",
    "res = tokenizer1(\"ACGCGCAG\", max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
    "res1 = tokenizer1([\"ACGCGCAG\", \"DACDAFV\"] , max_length=512,  padding=\"max_length\", return_tensors=\"pt\")\n",
    "res2 = tokenizer2([\"ACGCGCCDFAG\", \"CADCAEDF\"] , max_length=512,  padding=\"max_length\", return_tensors=\"pt\")\n",
    "model_encoder.eval()\n",
    "p1 = model_encoder(**res1).last_hidden_state\n",
    "p2 = model_encoder(**res2).last_hidden_state\n",
    "z = torch.concatenate([p1.mean(dim=1), p2.mean(dim=1)], dim=1)\n",
    "\n",
    "print(z.shape)\n",
    "model_encoder.config\n",
    "\n",
    "\n",
    "#datasets = datasets.map(lambda x: finetune_tokenize(tokenizer, x), batched=True)\n",
    "#tokenizer(datasets[\"train\"][\"text\"][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE USED: cuda\n",
      "NUM PROCESSES: 1\n",
      "Step 100 - Loss: 0.6549, AUROC: 0.5196, AUPRC: 0.5222\n",
      "Step 100 - AUROC val: 0.4994, AUPRC val: 0.5152\n",
      "Step 200 - Loss: 0.6820, AUROC: 0.5094, AUPRC: 0.5214\n",
      "Step 200 - AUROC val: 0.6207, AUPRC val: 0.6703\n",
      "Step 300 - Loss: 0.6445, AUROC: 0.6034, AUPRC: 0.5461\n",
      "Step 300 - AUROC val: 0.5646, AUPRC val: 0.6114\n",
      "Step 400 - Loss: 0.6008, AUROC: 0.5900, AUPRC: 0.5513\n",
      "Step 400 - AUROC val: 0.6143, AUPRC val: 0.6437\n",
      "Step 500 - Loss: 0.7124, AUROC: 0.6133, AUPRC: 0.5637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 - AUROC val: 0.5619, AUPRC val: 0.5998\n"
     ]
    }
   ],
   "source": [
    "train_interactions_file = \"train_interactions.csv\"\n",
    "val_interactions_file = \"val_interactions.csv\"\n",
    "\n",
    "train_parameters = {\"train_batch_size\": 4,\n",
    "                    \"device\": \"cuda\",\n",
    "                    \"learning_rate\": 5e-5,\n",
    "                    \"adam_epsilon\": 1e-6,\n",
    "                    \"gradient_accumulation_steps\": 16,\n",
    "                    \"num_training_steps\":500,\n",
    "                    \"log_performance_every\":100,\n",
    "                    }\n",
    "\n",
    "model = InteractionModel(model_encoder, 2)\n",
    "train_dataset = InputDataset(train_interactions_file, tokenizer1, tokenizer2)\n",
    "val_dataset = InputDataset(val_interactions_file, tokenizer1, tokenizer2)\n",
    "\n",
    "def evaluate_model(cpu_model, data_loader, thread_id):\n",
    "    cpu_model.eval() \n",
    "\n",
    "    auroc_metric = AUROC(task=\"multiclass\", num_classes=2)\n",
    "    auprc_metric = AveragePrecision(task=\"multiclass\", num_classes=2)\n",
    "    \n",
    "    eval_n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for source, targets in data_loader:\n",
    "      \n",
    "            if eval_n > 100:\n",
    "                break\n",
    "                        \n",
    "            output = cpu_model(source[0], source[1])\n",
    "            auroc_metric.update(output, targets)\n",
    "            auprc_metric.update(output, targets)\n",
    "\n",
    "            eval_n += 1\n",
    "\n",
    "    auroc_score = auroc_metric.compute()\n",
    "    auprc_score = auprc_metric.compute()\n",
    "    \n",
    "    print(f\"Step {thread_id} - AUROC val: {auroc_score:.4f}, AUPRC val: {auprc_score:.4f}\")\n",
    "      \n",
    "#add weight decay\n",
    "def finetune_and_evaluate(model):\n",
    "    deepspeed_plugin = DeepSpeedPlugin(zero_stage=3, \n",
    "                                       offload_param_device = \"cpu\",\n",
    "                                        offload_optimizer_device = \"cpu\"\n",
    "                                       )\n",
    "    accelerator = Accelerator(deepspeed_plugin=deepspeed_plugin, gradient_accumulation_steps=train_parameters[\"gradient_accumulation_steps\"])\n",
    "    device = accelerator.device\n",
    "\n",
    "    auroc_metric_train = AUROC(task=\"multiclass\", num_classes=2).to(device)\n",
    "    auprc_metric_train = AveragePrecision(task=\"multiclass\", num_classes=2).to(device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=train_parameters[\"learning_rate\"], eps=train_parameters[\"adam_epsilon\"], weight_decay=0.01)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_parameters[\"train_batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=True)\n",
    "\n",
    "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n",
    "    print(\"DEVICE USED:\", accelerator.device)\n",
    "    print(\"NUM PROCESSES:\", accelerator.num_processes)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    step = 0  # Initialize the step variable\n",
    "    max_steps = train_parameters[\"num_training_steps\"]\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    while step < max_steps:\n",
    "\n",
    "        for train_source, train_targets in train_loader:\n",
    "            with accelerator.accumulate(model):\n",
    "                output = model(train_source[0], train_source[1])\n",
    "                loss = criterion(output, train_targets)\n",
    "                grad_mean = []\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                auroc_metric_train.update(output, train_targets)\n",
    "                auprc_metric_train.update(output, train_targets)\n",
    "\n",
    "                # Compute metrics at a frequency of your choice (e.g., every 5 steps)\n",
    "                if (step + 1) % train_parameters[\"log_performance_every\"] == 0:\n",
    "                    auroc_score_train = auroc_metric_train.compute()\n",
    "                    auprc_score_train = auprc_metric_train.compute()\n",
    "                    auroc_metric_train.reset()\n",
    "                    auroc_metric_train.reset()\n",
    "                    # compute metrics for validation set\n",
    "                    cpu_model = copy.deepcopy(model).cpu()\n",
    "                    eval_thread = threading.Thread(\n",
    "                        target=evaluate_model, \n",
    "                        args=(cpu_model, val_loader, step+1)\n",
    "                    )\n",
    "                    eval_thread.start()\n",
    "\n",
    "            \n",
    "                optimizer.step()\n",
    "\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        #print(param.grad.mean())\n",
    "                        grad_mean.append(param.grad.mean().cpu().detach())\n",
    "                \n",
    "                #plt.plot(grad_mean)\n",
    "\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "                if (step + 1) % train_parameters[\"log_performance_every\"] == 0:\n",
    "                    print(f\"Step {step + 1} - Loss: {loss.item():.4f}, AUROC: {auroc_score_train:.4f}, AUPRC: {auprc_score_train:.4f}\")\n",
    "\n",
    "                step += 1\n",
    "\n",
    "                if step >= max_steps:\n",
    "                    break\n",
    "                \n",
    "\n",
    "finetune_and_evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-28T14:16:26.269012Z",
     "iopub.status.idle": "2024-10-28T14:16:26.269351Z",
     "shell.execute_reply": "2024-10-28T14:16:26.269194Z",
     "shell.execute_reply.started": "2024-10-28T14:16:26.269176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "longformer = LongformerForMaskedLM.from_pretrained('allenai/longformer-base-4096')\n",
    "longformer_base_tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-28T14:16:26.270269Z",
     "iopub.status.idle": "2024-10-28T14:16:26.270602Z",
     "shell.execute_reply": "2024-10-28T14:16:26.270450Z",
     "shell.execute_reply.started": "2024-10-28T14:16:26.270432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pretrain_and_evaluate(training_args, longformer, longformer_base_tokenizer, eval_only=False, model_path=None)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5959684,
     "sourceId": 9737243,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
